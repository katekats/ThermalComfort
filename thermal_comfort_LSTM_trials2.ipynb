{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11_2016_bedroom_ID1  11_2016_bedroom_ID12  11_2016_bedroom_ID14  \\\n",
      "492               16.936                18.746                19.983   \n",
      "493               16.623                18.746                20.046   \n",
      "494               16.372                18.809                20.108   \n",
      "495               16.122                18.934                20.108   \n",
      "496                3.000                 1.000                 1.000   \n",
      "\n",
      "     11_2016_bedroom_ID16  11_2016_bedroom_ID23  11_2016_bedroom_ID26  \\\n",
      "492                22.181                19.503                19.751   \n",
      "493                21.493                20.191                18.938   \n",
      "494                21.993                20.566                19.438   \n",
      "495                22.431                20.629                19.751   \n",
      "496                 1.000                 1.000                 1.000   \n",
      "\n",
      "     11_2016_bedroom_ID33  11_2016_bedroom_ID41  11_2016_living_room_ID1  \\\n",
      "492                14.333                16.436                   21.249   \n",
      "493                15.335                17.250                   21.499   \n",
      "494                16.024                15.811                   21.561   \n",
      "495                16.713                14.684                   21.999   \n",
      "496                 2.000                 1.000                    3.000   \n",
      "\n",
      "     11_2016_living_room_ID12  ...  03_2018_living_room_ID29  \\\n",
      "492                    21.643  ...                       0.0   \n",
      "493                    21.768  ...                       0.0   \n",
      "494                    21.580  ...                       0.0   \n",
      "495                    21.705  ...                       0.0   \n",
      "496                     1.000  ...                      -1.0   \n",
      "\n",
      "     03_2018_living_room_ID31  03_2018_living_room_ID32  \\\n",
      "492                       0.0                       0.0   \n",
      "493                       0.0                       0.0   \n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                       1.0                       0.0   \n",
      "\n",
      "     03_2018_living_room_ID34  03_2018_living_room_ID35  \\\n",
      "492                       0.0                       0.0   \n",
      "493                       0.0                       0.0   \n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                       0.0                      -1.0   \n",
      "\n",
      "     03_2018_living_room_ID36  03_2018_living_room_ID38  \\\n",
      "492                       0.0                       0.0   \n",
      "493                       0.0                       0.0   \n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                      -3.0                      -2.0   \n",
      "\n",
      "     03_2018_living_room_ID39  03_2018_living_room_ID40  \\\n",
      "492                       0.0                       0.0   \n",
      "493                       0.0                       0.0   \n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                      -1.0                      -1.0   \n",
      "\n",
      "     03_2018_living_room_ID43  \n",
      "492                       0.0  \n",
      "493                       0.0  \n",
      "494                       0.0  \n",
      "495                       0.0  \n",
      "496                       1.0  \n",
      "\n",
      "[5 rows x 268 columns]\n",
      "Epoch 1/40\n",
      "4/4 [==============================] - 19s 855ms/step - loss: 1.7979 - accuracy: 0.4000 - f1_score: 0.0000e+00 - val_loss: 1.7831 - val_accuracy: 0.2917 - val_f1_score: 0.1111\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 1.5522 - accuracy: 0.5000 - f1_score: 0.3116 - val_loss: 1.6833 - val_accuracy: 0.3333 - val_f1_score: 0.1132\n",
      "Epoch 3/40\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 1.4489 - accuracy: 0.5091 - f1_score: 0.2719 - val_loss: 1.6438 - val_accuracy: 0.3542 - val_f1_score: 0.0984\n",
      "Epoch 4/40\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 1.3733 - accuracy: 0.5227 - f1_score: 0.3440 - val_loss: 1.5714 - val_accuracy: 0.3750 - val_f1_score: 0.1290\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 1.3427 - accuracy: 0.5273 - f1_score: 0.3713 - val_loss: 1.4961 - val_accuracy: 0.3750 - val_f1_score: 0.1270\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 1.2781 - accuracy: 0.5455 - f1_score: 0.4669 - val_loss: 1.4299 - val_accuracy: 0.4583 - val_f1_score: 0.0968\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 1.2539 - accuracy: 0.5545 - f1_score: 0.4126 - val_loss: 1.4157 - val_accuracy: 0.5000 - val_f1_score: 0.1562\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 1.2182 - accuracy: 0.5591 - f1_score: 0.4370 - val_loss: 1.3218 - val_accuracy: 0.5625 - val_f1_score: 0.2069\n",
      "Epoch 9/40\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 1.1760 - accuracy: 0.5636 - f1_score: 0.4745 - val_loss: 1.3072 - val_accuracy: 0.5000 - val_f1_score: 0.1724\n",
      "Epoch 10/40\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 1.1464 - accuracy: 0.5682 - f1_score: 0.4927 - val_loss: 1.3136 - val_accuracy: 0.5000 - val_f1_score: 0.1791\n",
      "Epoch 11/40\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 1.1218 - accuracy: 0.5727 - f1_score: 0.5384 - val_loss: 1.2547 - val_accuracy: 0.5417 - val_f1_score: 0.2609\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 1.0983 - accuracy: 0.6000 - f1_score: 0.5109 - val_loss: 1.2823 - val_accuracy: 0.5000 - val_f1_score: 0.2090\n",
      "Epoch 13/40\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 1.0693 - accuracy: 0.5909 - f1_score: 0.5403 - val_loss: 1.1786 - val_accuracy: 0.5417 - val_f1_score: 0.2727\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 1.0227 - accuracy: 0.5955 - f1_score: 0.5728 - val_loss: 1.2069 - val_accuracy: 0.5208 - val_f1_score: 0.2609\n",
      "Epoch 15/40\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 1.0367 - accuracy: 0.6091 - f1_score: 0.5572 - val_loss: 1.1787 - val_accuracy: 0.5000 - val_f1_score: 0.4156\n",
      "Epoch 16/40\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 1.0221 - accuracy: 0.5955 - f1_score: 0.5791 - val_loss: 1.0631 - val_accuracy: 0.6667 - val_f1_score: 0.3437\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.9666 - accuracy: 0.6273 - f1_score: 0.5957 - val_loss: 1.1220 - val_accuracy: 0.5833 - val_f1_score: 0.3529\n",
      "Epoch 18/40\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.9656 - accuracy: 0.6318 - f1_score: 0.5652 - val_loss: 1.0497 - val_accuracy: 0.6458 - val_f1_score: 0.3051\n",
      "Epoch 19/40\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.9464 - accuracy: 0.6409 - f1_score: 0.5792 - val_loss: 1.0891 - val_accuracy: 0.5833 - val_f1_score: 0.3662\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.9003 - accuracy: 0.6500 - f1_score: 0.5963 - val_loss: 0.9594 - val_accuracy: 0.6458 - val_f1_score: 0.5070\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.9137 - accuracy: 0.6591 - f1_score: 0.5453 - val_loss: 1.1918 - val_accuracy: 0.5625 - val_f1_score: 0.4507\n",
      "Epoch 22/40\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.9022 - accuracy: 0.6455 - f1_score: 0.5470 - val_loss: 1.0786 - val_accuracy: 0.6875 - val_f1_score: 0.4444\n",
      "Epoch 23/40\n",
      "4/4 [==============================] - 0s 100ms/step - loss: 0.8627 - accuracy: 0.6591 - f1_score: 0.6317 - val_loss: 1.1054 - val_accuracy: 0.5833 - val_f1_score: 0.3684\n",
      "Epoch 24/40\n",
      "4/4 [==============================] - 0s 107ms/step - loss: 0.8566 - accuracy: 0.6909 - f1_score: 0.6227 - val_loss: 0.9219 - val_accuracy: 0.7083 - val_f1_score: 0.5429\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.8148 - accuracy: 0.7045 - f1_score: 0.6544 - val_loss: 1.0705 - val_accuracy: 0.5833 - val_f1_score: 0.4000\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.7948 - accuracy: 0.6636 - f1_score: 0.6364 - val_loss: 1.0148 - val_accuracy: 0.6875 - val_f1_score: 0.5135\n",
      "Epoch 27/40\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.8323 - accuracy: 0.7182 - f1_score: 0.6367 - val_loss: 1.0574 - val_accuracy: 0.5833 - val_f1_score: 0.5000\n",
      "Epoch 28/40\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.7417 - accuracy: 0.7318 - f1_score: 0.6785 - val_loss: 1.0644 - val_accuracy: 0.7083 - val_f1_score: 0.6098\n",
      "Epoch 29/40\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.7880 - accuracy: 0.7045 - f1_score: 0.6683 - val_loss: 0.9794 - val_accuracy: 0.6250 - val_f1_score: 0.5385\n",
      "Epoch 30/40\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.7328 - accuracy: 0.7455 - f1_score: 0.6858 - val_loss: 1.0826 - val_accuracy: 0.5833 - val_f1_score: 0.5476\n",
      "Epoch 31/40\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 0.7044 - accuracy: 0.7500 - f1_score: 0.7010 - val_loss: 1.1363 - val_accuracy: 0.6250 - val_f1_score: 0.5952\n",
      "Epoch 32/40\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.7234 - accuracy: 0.7136 - f1_score: 0.6560 - val_loss: 0.9207 - val_accuracy: 0.7292 - val_f1_score: 0.7356\n",
      "Epoch 33/40\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.6730 - accuracy: 0.7545 - f1_score: 0.7065 - val_loss: 1.0976 - val_accuracy: 0.7917 - val_f1_score: 0.6429\n",
      "Epoch 34/40\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.7261 - accuracy: 0.7364 - f1_score: 0.7218 - val_loss: 1.0230 - val_accuracy: 0.6042 - val_f1_score: 0.5854\n",
      "Epoch 35/40\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.6609 - accuracy: 0.7864 - f1_score: 0.7367 - val_loss: 0.7954 - val_accuracy: 0.6875 - val_f1_score: 0.7294\n",
      "Epoch 36/40\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.6551 - accuracy: 0.7409 - f1_score: 0.7341 - val_loss: 0.9694 - val_accuracy: 0.6250 - val_f1_score: 0.6190\n",
      "Epoch 37/40\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.6101 - accuracy: 0.8000 - f1_score: 0.7517 - val_loss: 1.0283 - val_accuracy: 0.6667 - val_f1_score: 0.5952\n",
      "Epoch 38/40\n",
      "4/4 [==============================] - 0s 146ms/step - loss: 0.6585 - accuracy: 0.7591 - f1_score: 0.7249 - val_loss: 0.9542 - val_accuracy: 0.6667 - val_f1_score: 0.6588\n",
      "Epoch 39/40\n",
      "4/4 [==============================] - 0s 122ms/step - loss: 0.6039 - accuracy: 0.7955 - f1_score: 0.7820 - val_loss: 0.9004 - val_accuracy: 0.6250 - val_f1_score: 0.6667\n",
      "Epoch 40/40\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 0.5503 - accuracy: 0.8136 - f1_score: 0.7648 - val_loss: 0.9599 - val_accuracy: 0.6667 - val_f1_score: 0.6512\n",
      "Test Accuracy: 66.67%\n",
      "Test F1 Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Flatten, Dense,LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove columns where their last row is null\n",
    "    df = df.drop(columns=df.columns[df.iloc[-1].isnull()])\n",
    "\n",
    "    # Remove columns with more than 80% NaN values and fill others with mean\n",
    "    threshold = 0.8 * len(df)\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Pad columns to have 496 rows, with last row unchanged\n",
    "    padding_len = 497 - len(df)\n",
    "    padding = pd.DataFrame(0, index=np.arange(padding_len), columns=df.columns)\n",
    "    df = pd.concat([padding, df], axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(*files):\n",
    "    dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df = preprocess_data(df)\n",
    "        # Add a prefix to each column name based on the file name\n",
    "        prefix = file.split('.')[0]  # Assuming the file name is '11_2016.csv', this gets '11_2016'\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "        \n",
    "        # Reset index after preprocessing to ensure unique indices\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all preprocessed dataframes\n",
    "    result = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Build the LSTM model for multi-class classification\n",
    "def build_classifier_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=290, input_shape=(1, 496), return_sequences=True)))     \n",
    "    #model.add(MultiHeadAttention(num_heads=2, key_dim=290))\n",
    "    #model.add(LayerNormalization(epsilon=1e-6))  # Layer normalization can help stabilize the outputs\n",
    "    # Add another LSTM layer with 120 units\n",
    "    model.add(LSTM(120, return_sequences=True))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # softmax for multi-class\n",
    "    model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['accuracy', f1_score])\n",
    "    return model\n",
    "\n",
    "# F1 Score Custom Metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_val = precision(y_true, y_pred)\n",
    "    recall_val = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_val * recall_val) / (precision_val + recall_val + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    # Load data\n",
    "    files = ['11_2016.csv', '12_2016.csv', '01_2017.csv', '02_2017.csv', '03_2018.csv',\n",
    "         '12_2017.csv', '01_2018.csv', '02_2018.csv', '03_2018.csv']\n",
    "    df = load_data(*files)\n",
    "    \n",
    "    # Handle missing values, for example, by replacing them\n",
    "    #df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    # Assuming single_file_result is your dataframe\n",
    "\n",
    "# Get unique values from row 496\n",
    "    unique_values_row_496 = df.iloc[496].unique()\n",
    "\n",
    "# Filter out the expected values\n",
    "    unexpected_values = [value for value in unique_values_row_496 if value not in [-3, -2, -1, 0, 1, 2, 3]]\n",
    "\n",
    "    print(df.tail())\n",
    "    df = df.T\n",
    "    caler2 = StandardScaler()\n",
    "    # Use the initial 200 rows for training\n",
    "    training_set = df.iloc[:220, :]\n",
    "    X_train = training_set.iloc[:, :-1].values\n",
    "    X_train = scaler2.fit_transform(X_train)\n",
    "    y_train = training_set.iloc[:, -1].values\n",
    "    # Reshape the X_train\n",
    "    num_samples_train, num_features_train = X_train.shape\n",
    "    X_train = np.reshape(X_train, (num_samples_train, 1, num_features_train))\n",
    "    # Prepare the testing set, using the remaining rows (from 200 to 268)\n",
    "    testing_set = df.iloc[220:268, :-1]\n",
    "    X_test = scaler2.fit_transform(testing_set)\n",
    "\n",
    "    \n",
    "    y_test = df.iloc[220:268, -1].values\n",
    "    #   Reshape the X_test\n",
    "    num_samples_test = X_test.shape[0]\n",
    "    X_test = np.reshape(X_test, (num_samples_test, 1, 496))\n",
    "    # Convert y_train and y_test to categorical\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = y_train.astype(int)\n",
    "    num_classes2 = len(np.unique(y_test))\n",
    "    y_test = y_test.astype(int)\n",
    "    # Convert labels to categorical\n",
    "    y_train = to_categorical(y_train, num_classes=7)  # Assuming 5 classes\n",
    "    y_test = to_categorical(y_test, num_classes=7)\n",
    "    # Train the model\n",
    "    model = build_classifier_model(496,7) # Assuming you have a function called build_classifier_model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=64)\n",
    "\n",
    "    # You can collect metrics or save models, weights, etc., during/after each iteration if required.\n",
    "\n",
    "    loss, accuracy, f1score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Accuracy: %.2f%%' % (accuracy * 100))\n",
    "    print('Test F1 Score: %.2f' % f1score)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 54.84%\n",
      "Average F1 Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Flatten, Dense,LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove columns where their last row is null\n",
    "    df = df.drop(columns=df.columns[df.iloc[-1].isnull()])\n",
    "\n",
    "    # Remove columns with more than 80% NaN values and fill others with mean\n",
    "    threshold = 0.8 * len(df)\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Pad columns to have 496 rows, with last row unchanged\n",
    "    padding_len = 497 - len(df)\n",
    "    padding = pd.DataFrame(0, index=np.arange(padding_len), columns=df.columns)\n",
    "    df = pd.concat([padding, df], axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(*files):\n",
    "    dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df = preprocess_data(df)\n",
    "        # Add a prefix to each column name based on the file name\n",
    "        prefix = file.split('.')[0]  # Assuming the file name is '11_2016.csv', this gets '11_2016'\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "        \n",
    "        # Reset index after preprocessing to ensure unique indices\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all preprocessed dataframes\n",
    "    result = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Build the LSTM model for multi-class classification\n",
    "def build_classifier_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=290, input_shape=(1, 496), return_sequences=True)))     \n",
    "    #model.add(MultiHeadAttention(num_heads=2, key_dim=290))\n",
    "    #model.add(LayerNormalization(epsilon=1e-6))  # Layer normalization can help stabilize the outputs\n",
    "    # Add another LSTM layer with 120 units\n",
    "    model.add(LSTM(120, return_sequences=True))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # softmax for multi-class\n",
    "    model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['accuracy', f1_score])\n",
    "    return model\n",
    "\n",
    "# F1 Score Custom Metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_val = precision(y_true, y_pred)\n",
    "    recall_val = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_val * recall_val) / (precision_val + recall_val + K.epsilon()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    files = ['11_2016.csv', '12_2016.csv', '01_2017.csv', '02_2017.csv', '03_2018.csv',\n",
    "             '12_2017.csv', '01_2018.csv', '02_2018.csv', '03_2018.csv']\n",
    "    df = load_data(*files)\n",
    "    df = df.T\n",
    "    # Setup KFold Cross Validation\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    f1scores = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(df):\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Data preprocessing\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Convert y_train and y_test to categorical\n",
    "        y_train, y_test = y_train.astype(int), y_test.astype(int)\n",
    "        y_train = to_categorical(y_train, num_classes=7)\n",
    "        y_test = to_categorical(y_test, num_classes=7)\n",
    "\n",
    "        # Reshape data for LSTM\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        # Train the model\n",
    "        model = build_classifier_model(X_train.shape[2], 7)\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=64, verbose=0)\n",
    "\n",
    "        # Evaluate the model\n",
    "        loss, accuracy, f1score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        accuracies.append(accuracy)\n",
    "        f1scores.append(f1score)\n",
    "\n",
    "    print('Average Accuracy: %.2f%%' % (np.mean(accuracies) * 100))\n",
    "    print('Average F1 Score: %.2f' % np.mean(f1scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12_2016_bedroom_ID1  12_2016_bedroom_ID12  12_2016_bedroom_ID14  \\\n",
      "494               14.430                19.872                20.359   \n",
      "495               14.305                19.935                20.296   \n",
      "496               14.180                19.935                20.108   \n",
      "497               14.180                19.935                20.108   \n",
      "498                1.000                -2.000                 1.000   \n",
      "\n",
      "     12_2016_bedroom_ID16  12_2016_bedroom_ID23  12_2016_bedroom_ID26  \\\n",
      "494                22.681                20.816                17.874   \n",
      "495                22.994                20.754                17.687   \n",
      "496                23.119                20.816                17.499   \n",
      "497                23.119                20.816                17.499   \n",
      "498                 1.000                 1.000                 1.000   \n",
      "\n",
      "     12_2016_bedroom_ID33  12_2016_bedroom_ID41  12_2016_living_room_ID1  \\\n",
      "494                18.027                17.563                   19.059   \n",
      "495                18.403                15.936                   19.247   \n",
      "496                19.091                15.122                   19.560   \n",
      "497                19.091                15.122                   19.560   \n",
      "498                -2.000                 1.000                    1.000   \n",
      "\n",
      "     12_2016_living_room_ID12  ...  03_2018_living_room_ID29  \\\n",
      "494                    21.643  ...                       0.0   \n",
      "495                    21.580  ...                       0.0   \n",
      "496                    21.580  ...                       0.0   \n",
      "497                    21.580  ...                       0.0   \n",
      "498                    -2.000  ...                      -1.0   \n",
      "\n",
      "     03_2018_living_room_ID31  03_2018_living_room_ID32  \\\n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                       0.0                       0.0   \n",
      "497                       0.0                       0.0   \n",
      "498                       1.0                       0.0   \n",
      "\n",
      "     03_2018_living_room_ID34  03_2018_living_room_ID35  \\\n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                       0.0                       0.0   \n",
      "497                       0.0                       0.0   \n",
      "498                       0.0                      -1.0   \n",
      "\n",
      "     03_2018_living_room_ID36  03_2018_living_room_ID38  \\\n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                       0.0                       0.0   \n",
      "497                       0.0                       0.0   \n",
      "498                      -3.0                      -2.0   \n",
      "\n",
      "     03_2018_living_room_ID39  03_2018_living_room_ID40  \\\n",
      "494                       0.0                       0.0   \n",
      "495                       0.0                       0.0   \n",
      "496                       0.0                       0.0   \n",
      "497                       0.0                       0.0   \n",
      "498                      -1.0                      -1.0   \n",
      "\n",
      "     03_2018_living_room_ID43  \n",
      "494                       0.0  \n",
      "495                       0.0  \n",
      "496                       0.0  \n",
      "497                       0.0  \n",
      "498                       1.0  \n",
      "\n",
      "[5 rows x 251 columns]\n",
      "[14.18, 19.935, 20.108, 23.119, 20.816, 17.499, 19.091, 15.122, 19.56, 21.58, 20.454, 23.039, 21.093, 19.937, 20.936, 16.34, 22.063, 13.177, 20.248, 21.109, 22.869, 20.066, 20.501, 20.781, 17.375, 16.994, 21.33, 20.517, 23.476, 20.968, 20.687, 22.313, 17.217, 22.688, 15.558, 19.622, 21.172, 21.868, 20.28, 13.744, 21.311, 21.393, 22.081, 22.038, 22.0, 16.904, 18.933, 16.435, 18.84, 21.375, 18.891, 17.266, 14.919, 10.259, 16.517, 21.559, 19.309, 15.673, 20.079, 19.341, 20.748, 18.746, 23.056, 13.059, 18.463, 13.873, 22.257, 20.503, 15.818, 17.358, 18.83, 21.188, 21.86, 19.796, 16.148, 17.32, 18.093, 14.804, 22.518, 22.335, 18.785, 19.25, 22.722, 21.281, 19.651, 20.68, 21.743, 18.03, 19.97, 20.528, 20.593, 18.554, 20.744, 18.804, 19.841, 19.904, 18.965, 18.643, 14.168, 10.196, 15.765, 20.621, 19.372, 13.731, 20.029, 17.557, 13.561, 22.757, 18.814, 21.876, 17.025, 16.381, 17.843, 13.864, 23.585, 15.154, 21.969, 20.555, 19.406, 19.842, 19.492, 20.243, 17.837, 18.33, 12.039, 5.996, 12.256, 18.681, 13.606, 17.903, 16.806, 22.243, 14.438, 21.279, 23.508, 19.123, 15.396, 15.065, 16.653, 9.603, 23.519, 16.907, 22.157, 19.367, 20.305, 18.78, 16.926]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 25398 into shape (51,1,497)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m \u001b[39m#   Reshape the X_test\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m num_samples_test \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m X_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(X_test, (num_samples_test, \u001b[39m1\u001b[39;49m, \u001b[39m497\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m \u001b[39m#print(num_samples_test)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \u001b[39m# Convert y_train and y_test to categorical\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_train))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ThermalComfort/myenv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[0;32m~/Desktop/ThermalComfort/myenv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 25398 into shape (51,1,497)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Flatten, Dense,LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove columns where their last row is null\n",
    "    df = df.drop(columns=df.columns[df.iloc[-1].isnull()])\n",
    "\n",
    "    # Remove columns with more than 80% NaN values and fill others with mean\n",
    "    threshold = 0.8 * len(df)\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Pad columns to have 496 rows, with last row unchanged\n",
    "    padding_len = 498 - len(df)\n",
    "    padding = pd.DataFrame(0, index=np.arange(padding_len), columns=df.columns)\n",
    "    df = pd.concat([padding, df], axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_mean_ext(year, month, house_id, temp_df):\n",
    "    # Create a mask to filter rows based on year and month\n",
    "    mask = (pd.to_datetime(temp_df['Date']).dt.year == year) & \\\n",
    "           (pd.to_datetime(temp_df['Date']).dt.month == month)\n",
    "    \n",
    "    # Check if the specific HouseID exists in the dataset\n",
    "    if house_id in temp_df['HouseID'].unique():\n",
    "        # If it exists, retrieve the t_mean_ext value for the given month, year, and HouseID\n",
    "        specific_id_values = temp_df[mask & (temp_df['HouseID'] == house_id)]['t_mean_ext'].values\n",
    "        if specific_id_values.size > 0:\n",
    "            return specific_id_values[0]\n",
    "    \n",
    "    # If the specific HouseID doesn't exist or there's no value for the given month and year, \n",
    "    # compute the mean t_mean_ext across all HouseIDs for that month and year\n",
    "    all_id_values = temp_df[mask]['t_mean_ext'].values\n",
    "    if all_id_values.size > 0:\n",
    "        return np.mean(all_id_values)\n",
    "    else:\n",
    "        # If no records are found at all for the given month and year, return 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(temperature_file, *files):\n",
    "    dataframes = []\n",
    "\n",
    "    # Load Temperatures.csv\n",
    "    temp_df = pd.read_csv(\"Winter_thermal_comfort_dataset/Temperatures.csv\")\n",
    "\n",
    "    for file in files:\n",
    "        # Extract year, month, and HouseID from the filename\n",
    "        split_name = file.split('.')[0].split('_')\n",
    "        year = int(split_name[1])\n",
    "        month = int(split_name[0])\n",
    "        house_id = int(split_name[-1].replace('ID', ''))\n",
    "\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df = preprocess_data(df)\n",
    "\n",
    "        # Add a prefix to each column name based on the file name\n",
    "        prefix = file.split('.')[0]\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "\n",
    "        # Get t_mean_ext value\n",
    "        mean_ext_value = get_mean_ext(year, month, house_id, temp_df)\n",
    "\n",
    "        # Construct the new row\n",
    "        new_row_data = []\n",
    "        for col in df.columns:\n",
    "            if f\"ID{house_id}\" in col and f\"_{month}_\" in col and f\"_{year}_\" in col:\n",
    "                # If the column matches the current ID, month, and year, set its value to t_mean_ext\n",
    "                new_row_data.append(mean_ext_value)\n",
    "            else:\n",
    "                # Otherwise, just duplicate the value from the second to last row of the original df\n",
    "                new_row_data.append(df.iloc[-2][col])\n",
    "        new_row = pd.DataFrame([new_row_data], columns=df.columns)\n",
    "\n",
    "        # Split the dataframe, concatenate, and reset index\n",
    "        df1 = df.iloc[:-1]\n",
    "        df2 = df.iloc[-1:]\n",
    "        df = pd.concat([df1, new_row, df2], axis=0).reset_index(drop=True)\n",
    "\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all the dataframes in the list\n",
    "    result = pd.concat(dataframes, axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the LSTM model for multi-class classification\n",
    "def build_classifier_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=290, input_shape=(1, 497), return_sequences=True)))     \n",
    "    #model.add(MultiHeadAttention(num_heads=2, key_dim=290))\n",
    "    #model.add(LayerNormalization(epsilon=1e-6))  # Layer normalization can help stabilize the outputs\n",
    "    # Add another LSTM layer with 120 units\n",
    "    model.add(LSTM(120, return_sequences=True))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # softmax for multi-class\n",
    "    model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['accuracy', f1_score])\n",
    "    return model\n",
    "\n",
    "# F1 Score Custom Metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_val = precision(y_true, y_pred)\n",
    "    recall_val = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_val * recall_val) / (precision_val + recall_val + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    # Load data\n",
    "    files = ['11_2016.csv', '12_2016.csv', '01_2017.csv', '02_2017.csv', '03_2018.csv',\n",
    "         '12_2017.csv', '01_2018.csv', '02_2018.csv', '03_2018.csv']\n",
    "    df = load_data(*files)\n",
    "    \n",
    "    # Handle missing values, for example, by replacing them\n",
    "    #df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    # Assuming single_file_result is your dataframe\n",
    "    print(df.tail())\n",
    "# Get unique values from row 496\n",
    "    unique_values_row_496 = df.iloc[497].unique()\n",
    "\n",
    "# Filter out the expected values\n",
    "    unexpected_values = [value for value in unique_values_row_496 if value not in [-3, -2, -1, 0, 1, 2, 3]]\n",
    "    print(unexpected_values)\n",
    "    df = df.T\n",
    "\n",
    "    caler2 = StandardScaler()\n",
    "    # Use the initial 200 rows for training\n",
    "    training_set = df.iloc[:200, :]\n",
    "    X_train = training_set.iloc[:, :-1].values\n",
    "    X_train = scaler2.fit_transform(X_train)\n",
    "    y_train = training_set.iloc[:, -1].values\n",
    "    # Reshape the X_train\n",
    "    num_samples_train, num_features_train = X_train.shape\n",
    "    X_train = np.reshape(X_train, (num_samples_train, 1, num_features_train))\n",
    "    # Prepare the testing set, using the remaining rows (from 200 to 268)\n",
    "    testing_set = df.iloc[200:268, :-1]\n",
    "    X_test = scaler2.fit_transform(testing_set)\n",
    "\n",
    "    y_test = df.iloc[200:268, -1].values\n",
    "    #   Reshape the X_test\n",
    "    num_samples_test = X_test.shape[0]\n",
    "\n",
    "    X_test = np.reshape(X_test, (num_samples_test, 1, 497))\n",
    "    #print(num_samples_test)\n",
    "    # Convert y_train and y_test to categorical\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = y_train.astype(int)\n",
    "    num_classes2 = len(np.unique(y_test))\n",
    "    y_test = y_test.astype(int)\n",
    "    # Convert labels to categorical\n",
    "    #print(np.unique(y_train))\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes=7)  # Assuming 7 classes\n",
    "    y_test = to_categorical(y_test, num_classes=7)\n",
    "    # Train the model\n",
    "    model = build_classifier_model(497,7) # Assuming you have a function called build_classifier_model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=64)\n",
    "\n",
    "    # You can collect metrics or save models, weights, etc., during/after each iteration if required.\n",
    "\n",
    "    loss, accuracy, f1score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Accuracy: %.2f%%' % (accuracy * 100))\n",
    "    print('Test F1 Score: %.2f' % f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
