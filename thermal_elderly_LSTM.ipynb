{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Flatten, Dense,LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove columns where their last row is null\n",
    "    df = df.drop(columns=df.columns[df.iloc[-1].isnull()])\n",
    "\n",
    "    # Remove columns with more than 80% NaN values and fill others with mean\n",
    "    threshold = 0.8 * len(df)\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Pad columns to have 496 rows, with last row unchanged\n",
    "    padding_len = 497 - len(df)\n",
    "    padding = pd.DataFrame(0, index=np.arange(padding_len), columns=df.columns)\n",
    "    df = pd.concat([padding, df], axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(*files):\n",
    "    dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df = preprocess_data(df)\n",
    "        # Add a prefix to each column name based on the file name\n",
    "        prefix = file.split('.')[0]  # Assuming the file name is '11_2016.csv', this gets '11_2016'\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "        \n",
    "        # Reset index after preprocessing to ensure unique indices\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all preprocessed dataframes\n",
    "    result = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Build the LSTM model for multi-class classification\n",
    "def build_classifier_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=290, input_shape=(1, 496), return_sequences=True)))     \n",
    "    #model.add(MultiHeadAttention(num_heads=2, key_dim=290))\n",
    "    #model.add(LayerNormalization(epsilon=1e-6))  # Layer normalization can help stabilize the outputs\n",
    "    # Add another LSTM layer with 120 units\n",
    "    model.add(LSTM(120, return_sequences=True))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # softmax for multi-class\n",
    "    model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['accuracy', f1_score])\n",
    "    return model\n",
    "\n",
    "# F1 Score Custom Metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_val = precision(y_true, y_pred)\n",
    "    recall_val = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_val * recall_val) / (precision_val + recall_val + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    # Load data\n",
    "    files = ['11_2016.csv', '12_2016.csv', '01_2017.csv', '02_2017.csv', '03_2018.csv',\n",
    "         '12_2017.csv', '01_2018.csv', '02_2018.csv', '03_2018.csv']\n",
    "    df = load_data(*files)\n",
    "    \n",
    "    # Handle missing values, for example, by replacing them\n",
    "    #df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    # Assuming single_file_result is your dataframe\n",
    "\n",
    "# Get unique values from row 496\n",
    "    unique_values_row_496 = df.iloc[496].unique()\n",
    "\n",
    "# Filter out the expected values\n",
    "    unexpected_values = [value for value in unique_values_row_496 if value not in [-3, -2, -1, 0, 1, 2, 3]]\n",
    "\n",
    "    \n",
    "    df = df.T\n",
    "    caler2 = StandardScaler()\n",
    "    # Use the initial 200 rows for training\n",
    "    training_set = df.iloc[:220, :]\n",
    "    X_train = training_set.iloc[:, :-1].values\n",
    "    X_train = scaler2.fit_transform(X_train)\n",
    "    y_train = training_set.iloc[:, -1].values\n",
    "    # Reshape the X_train\n",
    "    num_samples_train, num_features_train = X_train.shape\n",
    "    X_train = np.reshape(X_train, (num_samples_train, 1, num_features_train))\n",
    "    # Prepare the testing set, using the remaining rows (from 200 to 268)\n",
    "    testing_set = df.iloc[220:268, :-1]\n",
    "    X_test = scaler2.fit_transform(testing_set)\n",
    "\n",
    "    \n",
    "    y_test = df.iloc[220:268, -1].values\n",
    "    #   Reshape the X_test\n",
    "    num_samples_test = X_test.shape[0]\n",
    "    X_test = np.reshape(X_test, (num_samples_test, 1, 496))\n",
    "    # Convert y_train and y_test to categorical\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = y_train.astype(int)\n",
    "    num_classes2 = len(np.unique(y_test))\n",
    "    y_test = y_test.astype(int)\n",
    "    # Convert labels to categorical\n",
    "    y_train = to_categorical(y_train, num_classes=7)  # Assuming 5 classes\n",
    "    y_test = to_categorical(y_test, num_classes=7)\n",
    "    # Train the model\n",
    "    model = build_classifier_model(496,7) # Assuming you have a function called build_classifier_model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=64)\n",
    "\n",
    "    # You can collect metrics or save models, weights, etc., during/after each iteration if required.\n",
    "\n",
    "    loss, accuracy, f1score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Accuracy: %.2f%%' % (accuracy * 100))\n",
    "    print('Test F1 Score: %.2f' % f1score)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible indexer with Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     \u001b[39m# Load data\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m     \u001b[39m# Load data\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m     files \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m11_2016.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m12_2016.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m01_2017.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m02_2017.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m03_2018.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m          \u001b[39m'\u001b[39m\u001b[39m12_2017.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m01_2018.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m02_2018.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m03_2018.csv\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     df \u001b[39m=\u001b[39m load_data(\u001b[39m*\u001b[39;49mfiles)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     \u001b[39m# Handle missing values, for example, by replacing them\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     \u001b[39m#df.fillna(method='ffill', inplace=True)  # Forward fill\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     \u001b[39m# Assuming single_file_result is your dataframe\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# Get unique values from row 496\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     unique_values_row_496 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[\u001b[39m496\u001b[39m]\u001b[39m.\u001b[39munique()\n",
      "\u001b[1;32m/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     mean_ext_value \u001b[39m=\u001b[39m get_mean_ext(year, month, house_id, temp_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# Insert the mean_ext_value right before the last row of the column\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     df\u001b[39m.\u001b[39;49mloc[df\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, column] \u001b[39m=\u001b[39m mean_ext_value\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Reset index after preprocessing to ensure unique indices\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aikaterinikatsarou/Desktop/ThermalComfort/ThermalComfort/thermal_elderly_LSTM.ipynb#X43sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m df\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/ThermalComfort/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    848\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[0;32m--> 849\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[0;32m~/Desktop/ThermalComfort/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:1835\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[39mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1834\u001b[0m     \u001b[39m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1835\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[1;32m   1836\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1837\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/Desktop/ThermalComfort/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:1856\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(value, ABCSeries) \u001b[39mand\u001b[39;00m name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   1854\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m Series\n\u001b[0;32m-> 1856\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_align_series(indexer, Series(value))\n\u001b[1;32m   1858\u001b[0m \u001b[39m# Ensure we have something we can iterate over\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m info_axis \u001b[39m=\u001b[39m indexer[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/ThermalComfort/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:2299\u001b[0m, in \u001b[0;36m_iLocIndexer._align_series\u001b[0;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   2297\u001b[0m     \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39mreindex(ax)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 2299\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIncompatible indexer with Series\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible indexer with Series"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Flatten, Dense,LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove columns where their last row is null\n",
    "    df = df.drop(columns=df.columns[df.iloc[-1].isnull()])\n",
    "\n",
    "    # Remove columns with more than 80% NaN values and fill others with mean\n",
    "    threshold = 0.8 * len(df)\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Pad columns to have 496 rows, with last row unchanged\n",
    "    padding_len = 497 - len(df)\n",
    "    padding = pd.DataFrame(0, index=np.arange(padding_len), columns=df.columns)\n",
    "    df = pd.concat([padding, df], axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_mean_ext(year, month, house_id, temp_df):\n",
    "    # Filter the Temperatures dataframe based on year, month, and HouseID\n",
    "    mask = (pd.to_datetime(temp_df['Date']).dt.year == year) & \\\n",
    "           (pd.to_datetime(temp_df['Date']).dt.month == month) & \\\n",
    "           (temp_df['HouseID'] == house_id)\n",
    "    return temp_df[mask]['t_mean_ext'].values[0]\n",
    "\n",
    "def load_data(temperature_file, *files):\n",
    "    dataframes = []\n",
    "    \n",
    "    # Load Temperatures.csv\n",
    "    temp_df = pd.read_csv(\"Winter_thermal_comfort_dataset/Temperatures.csv\")\n",
    "    \n",
    "    for file in files:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df = preprocess_data(df)\n",
    "        \n",
    "        # Add a prefix to each column name based on the file name\n",
    "        prefix = file.split('.')[0]  # Assuming the file name is '11_2016.csv', this gets '11_2016'\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "        \n",
    "        for column in df.columns:\n",
    "            # Extract year, month, and HouseID from the column name\n",
    "            split_name = column.split('_')\n",
    "            year = int(split_name[1])\n",
    "            month = int(split_name[0])\n",
    "            house_id = int(split_name[-1].replace('ID', ''))\n",
    "            \n",
    "            # Get t_mean_ext value\n",
    "            mean_ext_value = get_mean_ext(year, month, house_id, temp_df)\n",
    "            \n",
    "            # Insert the mean_ext_value right before the last row of the column\n",
    "            df.loc[df.shape[0]-2, column] = mean_ext_value\n",
    "        \n",
    "        # Reset index after preprocessing to ensure unique indices\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all preprocessed dataframes\n",
    "    result = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the LSTM model for multi-class classification\n",
    "def build_classifier_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=290, input_shape=(1, 496), return_sequences=True)))     \n",
    "    #model.add(MultiHeadAttention(num_heads=2, key_dim=290))\n",
    "    #model.add(LayerNormalization(epsilon=1e-6))  # Layer normalization can help stabilize the outputs\n",
    "    # Add another LSTM layer with 120 units\n",
    "    model.add(LSTM(120, return_sequences=True))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # softmax for multi-class\n",
    "    model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['accuracy', f1_score])\n",
    "    return model\n",
    "\n",
    "# F1 Score Custom Metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_val = precision(y_true, y_pred)\n",
    "    recall_val = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_val * recall_val) / (precision_val + recall_val + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    # Load data\n",
    "    files = ['11_2016.csv', '12_2016.csv', '01_2017.csv', '02_2017.csv', '03_2018.csv',\n",
    "         '12_2017.csv', '01_2018.csv', '02_2018.csv', '03_2018.csv']\n",
    "    df = load_data(*files)\n",
    "    \n",
    "    # Handle missing values, for example, by replacing them\n",
    "    #df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    # Assuming single_file_result is your dataframe\n",
    "\n",
    "# Get unique values from row 496\n",
    "    unique_values_row_496 = df.iloc[496].unique()\n",
    "\n",
    "# Filter out the expected values\n",
    "    unexpected_values = [value for value in unique_values_row_496 if value not in [-3, -2, -1, 0, 1, 2, 3]]\n",
    "\n",
    "    \n",
    "    df = df.T\n",
    "    caler2 = StandardScaler()\n",
    "    # Use the initial 200 rows for training\n",
    "    training_set = df.iloc[:200, :]\n",
    "    X_train = training_set.iloc[:, :-1].values\n",
    "    X_train = scaler2.fit_transform(X_train)\n",
    "    y_train = training_set.iloc[:, -1].values\n",
    "    # Reshape the X_train\n",
    "    num_samples_train, num_features_train = X_train.shape\n",
    "    X_train = np.reshape(X_train, (num_samples_train, 1, num_features_train))\n",
    "    # Prepare the testing set, using the remaining rows (from 200 to 268)\n",
    "    testing_set = df.iloc[200:268, :-1]\n",
    "    X_test = scaler2.fit_transform(testing_set)\n",
    "\n",
    "    \n",
    "    y_test = df.iloc[200:268, -1].values\n",
    "    #   Reshape the X_test\n",
    "    num_samples_test = X_test.shape[0]\n",
    "    X_test = np.reshape(X_test, (num_samples_test, 1, 496))\n",
    "    # Convert y_train and y_test to categorical\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = y_train.astype(int)\n",
    "    num_classes2 = len(np.unique(y_test))\n",
    "    y_test = y_test.astype(int)\n",
    "    # Convert labels to categorical\n",
    "    y_train = to_categorical(y_train, num_classes=7)  # Assuming 5 classes\n",
    "    y_test = to_categorical(y_test, num_classes=7)\n",
    "    # Train the model\n",
    "    model = build_classifier_model(496,7) # Assuming you have a function called build_classifier_model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=64)\n",
    "\n",
    "    # You can collect metrics or save models, weights, etc., during/after each iteration if required.\n",
    "\n",
    "    loss, accuracy, f1score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Accuracy: %.2f%%' % (accuracy * 100))\n",
    "    print('Test F1 Score: %.2f' % f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
