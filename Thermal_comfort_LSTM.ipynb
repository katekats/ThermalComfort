{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Flatten, Dense,LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Remove columns where their last row is null\n",
    "    df = df.drop(columns=df.columns[df.iloc[-1].isnull()])\n",
    "\n",
    "    # Remove columns with more than 80% NaN values and fill others with mean\n",
    "    threshold = 0.8 * len(df)\n",
    "    df = df.dropna(thresh=threshold, axis=1)\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Pad columns to have 496 rows, with last row unchanged\n",
    "    padding_len = 497 - len(df)\n",
    "    padding = pd.DataFrame(0, index=np.arange(padding_len), columns=df.columns)\n",
    "    df = pd.concat([padding, df], axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(*files):\n",
    "    dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        # Load dataframe\n",
    "        df = pd.read_csv(file, header=0)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        df = preprocess_data(df)\n",
    "        # Add a prefix to each column name based on the file name\n",
    "        prefix = file.split('.')[0]  # Assuming the file name is '11_2016.csv', this gets '11_2016'\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "        \n",
    "        # Reset index after preprocessing to ensure unique indices\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all preprocessed dataframes\n",
    "    result = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Build the LSTM model for multi-class classification\n",
    "def build_classifier_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=290, input_shape=(1, 496), return_sequences=True)))     \n",
    "    #model.add(MultiHeadAttention(num_heads=2, key_dim=290))\n",
    "    #model.add(LayerNormalization(epsilon=1e-6))  # Layer normalization can help stabilize the outputs\n",
    "    # Add another LSTM layer with 120 units\n",
    "    model.add(LSTM(120, return_sequences=True))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # softmax for multi-class\n",
    "    model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['accuracy', f1_score])\n",
    "    return model\n",
    "\n",
    "# F1 Score Custom Metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_val = precision(y_true, y_pred)\n",
    "    recall_val = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_val * recall_val) / (precision_val + recall_val + K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    # Load data\n",
    "    files = ['11_2016.csv', '12_2016.csv', '01_2017.csv', '02_2017.csv', '03_2018.csv',\n",
    "         '12_2017.csv', '01_2018.csv', '02_2018.csv', '03_2018.csv']\n",
    "    df = load_data(*files)\n",
    "    \n",
    "    # Handle missing values, for example, by replacing them\n",
    "    #df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    # Assuming single_file_result is your dataframe\n",
    "\n",
    "# Get unique values from row 496\n",
    "    unique_values_row_496 = df.iloc[496].unique()\n",
    "\n",
    "# Filter out the expected values\n",
    "    unexpected_values = [value for value in unique_values_row_496 if value not in [-3, -2, -1, 0, 1, 2, 3]]\n",
    "\n",
    "    print(df.tail())\n",
    "    df = df.T\n",
    "    caler2 = StandardScaler()\n",
    "    # Use the initial 200 rows for training\n",
    "    training_set = df.iloc[:220, :]\n",
    "    X_train = training_set.iloc[:, :-1].values\n",
    "    X_train = scaler2.fit_transform(X_train)\n",
    "    y_train = training_set.iloc[:, -1].values\n",
    "    # Reshape the X_train\n",
    "    num_samples_train, num_features_train = X_train.shape\n",
    "    X_train = np.reshape(X_train, (num_samples_train, 1, num_features_train))\n",
    "    # Prepare the testing set, using the remaining rows (from 200 to 268)\n",
    "    testing_set = df.iloc[220:268, :-1]\n",
    "    X_test = scaler2.fit_transform(testing_set)\n",
    "\n",
    "    \n",
    "    y_test = df.iloc[220:268, -1].values\n",
    "    #   Reshape the X_test\n",
    "    num_samples_test = X_test.shape[0]\n",
    "    X_test = np.reshape(X_test, (num_samples_test, 1, 496))\n",
    "    # Convert y_train and y_test to categorical\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = y_train.astype(int)\n",
    "    num_classes2 = len(np.unique(y_test))\n",
    "    y_test = y_test.astype(int)\n",
    "    # Convert labels to categorical\n",
    "    y_train = to_categorical(y_train, num_classes=7)  # Assuming 5 classes\n",
    "    y_test = to_categorical(y_test, num_classes=7)\n",
    "    # Train the model\n",
    "    model = build_classifier_model(496,7) # Assuming you have a function called build_classifier_model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=40, batch_size=64)\n",
    "\n",
    "    # You can collect metrics or save models, weights, etc., during/after each iteration if required.\n",
    "\n",
    "    loss, accuracy, f1score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Accuracy: %.2f%%' % (accuracy * 100))\n",
    "    print('Test F1 Score: %.2f' % f1score)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
